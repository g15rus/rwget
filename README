# RWget

Задача:

Есть асинхронный веб-сервер tornado, который также является фреймворком для написания веб-сайтов: http://www.tornadoweb.org/ Реализуйте при помощи него "рекурсивный асинхронный WGET".

На вход предоставляется URL. На выходе необходимо иметь древовидную структуру URL'ов, пройденных программой: все URLы, которые можно достичь последовательным переходом с главной страницы. Если запрашиваемый URL выдает HTML-документ, то программа должна рекурсивно пройтись по нему. Глубину рекурсии можно ограничить значением 20.

## Немного мыслей

Рекурсия глубиной 20 — это при наличии на каждой опрашиваемой странице хотя бы по 10 ссылок, в результате получим 10 ^ 20 ссылок.
Чтобы не фетчить так глубоко, ограничил количеством обработанных ссылок.

## Реализация

Скрипт реалищован в виде командной строки.

    python ./rwget.py -u http://hh.ru
    
По умолчанию опрашивает 1000 ссылок. Если нужно больше, то есть параметр -m, в котором можно указать количество урлов на выходе.
    
    python ./rwget.py -u http://hh.ru -m 5000 